{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYbRt5Un677i"
      },
      "outputs": [],
      "source": [
        "# grid_search_optimized_with_recommendation.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import pickle\n",
        "from lightfm import LightFM\n",
        "from lightfm.data import Dataset\n",
        "from lightfm.evaluation import precision_at_k, auc_score\n",
        "from lightfm.cross_validation import random_train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "file_path = 'webtoon_originals_en.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df = df[['title', 'genre', 'authors', 'subscribers', 'views', 'likes', 'rating', 'synopsis']].dropna()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "df['genre'] = df['genre'].apply(clean_text)\n",
        "df['authors'] = df['authors'].fillna('unknown').apply(clean_text)\n",
        "df['synopsis'] = df['synopsis'].fillna('').apply(clean_text)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df[['views', 'likes', 'subscribers']] = scaler.fit_transform(df[['views', 'likes', 'subscribers']])\n",
        "df['implicit_score'] = df['views'] * 0.5 + df['likes'] * 0.3 + df['subscribers'] * 0.2 + 0.1 * (df['rating'] / 10)\n",
        "\n",
        "num_users = 100\n",
        "user_ids = [f\"user_{i}\" for i in range(num_users)]\n",
        "webtoons = df['title'].tolist()\n",
        "interaction_data = []\n",
        "preferred_genres = ['romance', 'action', 'comedy', 'fantasy']\n",
        "\n",
        "for i, user in enumerate(user_ids):\n",
        "    genre = preferred_genres[i % len(preferred_genres)]\n",
        "    preferred_titles = df[df['genre'].str.contains(genre)]['title'].tolist()\n",
        "    if len(preferred_titles) < 10:\n",
        "        preferred_titles = webtoons\n",
        "    liked = random.sample(preferred_titles, min(80, len(preferred_titles)))\n",
        "    for title in liked:\n",
        "        score = df[df['title'] == title]['implicit_score'].values[0]\n",
        "        interaction_data.append((user, title, score))\n",
        "\n",
        "# Langsung generate embedding\n",
        "model_bert = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
        "synopsis_embeddings = model_bert.encode(df['synopsis'].tolist(), batch_size=128, show_progress_bar=True)\n",
        "\n",
        "# Dataset setup\n",
        "dataset = Dataset()\n",
        "dataset.fit(users=user_ids, items=df['title'])\n",
        "\n",
        "df['item_features'] = df['genre'] + ',' + df['authors']\n",
        "embedding_features = [f'synopsis_dim_{i}' for i in range(synopsis_embeddings.shape[1])]\n",
        "embedding_df = pd.DataFrame(synopsis_embeddings, columns=embedding_features)\n",
        "df = pd.concat([df.reset_index(drop=True), embedding_df.reset_index(drop=True)], axis=1)\n",
        "for feat in embedding_features:\n",
        "    df['item_features'] += ',' + feat\n",
        "\n",
        "df['item_features'] = df['item_features'].fillna('unknown').astype(str)\n",
        "all_item_features = set()\n",
        "for feats in df['item_features']:\n",
        "    all_item_features.update(feats.split(','))\n",
        "\n",
        "dataset.fit_partial(items=df['title'], item_features=all_item_features)\n",
        "interactions, _ = dataset.build_interactions(interaction_data)\n",
        "item_features = dataset.build_item_features(((title, feats.split(',')) for title, feats in zip(df['title'], df['item_features'])))\n",
        "\n",
        "train, test = random_train_test_split(interactions, test_percentage=0.2)\n",
        "\n",
        "# Train final model\n",
        "model = LightFM(loss='warp', learning_rate=0.05, no_components=64)\n",
        "model.fit(train, item_features=item_features, epochs=30, num_threads=2)\n",
        "\n",
        "# Top items DataFrame untuk rekomendasi\n",
        "top_items = df[['title']].reset_index(drop=True)\n",
        "\n",
        "# Cold-start: Content-based recommendation\n",
        "def content_based_recommendation(input_titles, top_n=10):\n",
        "    idx_inputs = [top_items[top_items['title'] == t].index[0] for t in input_titles if t in top_items['title'].values]\n",
        "    if not idx_inputs:\n",
        "        return \"Judul tidak ditemukan.\"\n",
        "    input_vectors = synopsis_embeddings[idx_inputs]\n",
        "    avg_vector = np.mean(input_vectors, axis=0).reshape(1, -1)\n",
        "    similarities = np.dot(synopsis_embeddings, avg_vector.T).flatten()\n",
        "    for idx in idx_inputs:\n",
        "        similarities[idx] = -1\n",
        "    top_indices = np.argsort(-similarities)[:top_n]\n",
        "    return top_items['title'].iloc[top_indices].tolist()\n",
        "\n",
        "# Hybrid recommendation\n",
        "def hybrid_recommendation(user_id, input_titles, top_n=10):\n",
        "    user_idx = 0  # hanya user_0\n",
        "    item_labels = list(top_items['title'])\n",
        "    idx_inputs = [item_labels.index(t) for t in input_titles if t in item_labels]\n",
        "    if not idx_inputs:\n",
        "        return \"Judul tidak ditemukan.\"\n",
        "    scores = model.predict(user_ids=user_idx, item_ids=np.arange(len(item_labels)), item_features=item_features)\n",
        "    for idx in idx_inputs:\n",
        "        scores[idx] = -np.inf\n",
        "    top_items_idx = np.argsort(-scores)[:top_n]\n",
        "    return [item_labels[i] for i in top_items_idx]\n",
        "\n",
        "# Adaptif input menu\n",
        "user_history_file = 'user_input_history.txt'\n",
        "user_id = 'user_0'\n",
        "input_titles = []\n",
        "\n",
        "print(\"\\nðŸŽ¯ Sistem Rekomendasi Webtoon\")\n",
        "genre_input = input(\"Masukkan genre favoritmu (e.g. romance, action): \").lower()\n",
        "\n",
        "while True:\n",
        "    title = input(\"Masukkan judul Webtoon favoritmu (atau 'exit'): \")\n",
        "    if title.lower() == 'exit':\n",
        "        break\n",
        "    if title not in top_items['title'].values:\n",
        "        print(f\"Judul '{title}' tidak ditemukan.\")\n",
        "        continue\n",
        "    input_titles.append(title)\n",
        "    with open(user_history_file, 'a') as f:\n",
        "        f.write(f\"{user_id},{title}\\n\")\n",
        "\n",
        "    if len(input_titles) < 3:\n",
        "        print(\"\\nðŸ” Rekomendasi awal (cold start):\")\n",
        "        rekomendasi = content_based_recommendation(input_titles)\n",
        "    else:\n",
        "        print(\"\\nðŸ¤– Rekomendasi campuran (hybrid):\")\n",
        "        rekomendasi = hybrid_recommendation(user_id, input_titles)\n",
        "\n",
        "    for i, rec in enumerate(rekomendasi, 1):\n",
        "        print(f\"{i}. {rec}\")\n"
      ]
    }
  ]
}